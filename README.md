# Document-Level Embeddings for Duplicate GitHub Issue Detection
## Research paper: [Duplicate_Ticket_Classification.pdf](https://github.com/kianhk6/Duplicate-GitHub-Ticket-Classification/files/15084423/Duplicate_Ticket_Classification.pdf)
## please for current developement check the improvement from baseline section

## Setup
### Quickstart
Clone this repository

Setting up your virtual environment and installing the required packages:

```
python3 -m venv venv
source venv/bin/activate
pip3 install -r requirements.txt 
```
### Downloading the Dataset
Download the [mozilla_firefox.zip](https://github.com/logpai/bughub/tree/master/Firefox) file, create a `data` folder in the root level of this repository and place the mozilla_firefox.csv file inside of it. Note that it's been manually untracked by the gitignore file because it exceeds the SFU github file limit of 100mb. 

## Overview

Document embeddings serve a purpose in a multitude of tasks including information retrieval, clustering, and pair classification, by enabling semantic similarity comparison between two documents via the cosine similarity operator. This property makes document embeddings useful for detecting duplicate issues in large repositories with backlogs of thousands of issues as an extension of the pair classification task. 

We evaluate 8 document embeddings models for the duplicate detection task within a repository of GitHub issues by computing pair-wise comparisons between issues and evaluating the pair-wise duplicate classification via the F1 metric a number of thresholds. 

We find that the training objective and task, as well as the fine-tune data have the biggest effect on the model's performance, confirming the findings of [Muennighoff et al](https://arxiv.org/abs/2210.07316) that no embedding model dominates across all tasks. 

## Pipeline

To make sure your environment is set up correctly and everything is running as expected, run the `check.ipynb` notebook in the master folder.

If everything executes as expected, you can run the `evaluate_multiple_models.ipynb` notebook.

### Data Configuration
Centralizes configuration settings in `configurations.py`:
- Data paths: `DATA_PATH`, `DATASET_FILE`
- Models to evaluate: `MODEL_NAMES`
- Other model-specific settings

### Extract-Transform-Load (ETL) Data
Manages data preprocessing, transformation, and loading using `ETLProcessor` class in `extract_transform_load.py`:
- Load raw data from CSV files
- Clean and prepare the dataset
- Save the processed dataset in a pickle file

### Embedding Generation
Facilitates the generation of text embeddings via `EmbeddingsGenerator`, `DatasetManager`, and `EmbeddingsPipeline` classes in :
- Handle embedding generation
- Load and preprocess datasets
- Orchestrate the embedding generation and saving process

### Embedding Evaluation
Evaluates embedding performance using `EmbeddingEvaluator`:
- Calculate similarity matrix
- Compute F1 score, precision, recall, cross-entropy loss, and ROC-AUC

### Evaluating Multiple Models
Automates the entire evaluation process in a Jupyter Notebook:
- Setup necessary directories
- Prepare CSV for storing metrics
- Execute data loading, embedding generation, and evaluation
- Store evaluation metrics

### Pipeline Flow
![Pipeline Architecture](Model%20Architecture.png)


## Results

![F1 Scores](output.png)

# Improvement from Baseline

### Classification of Untriaged Tickets
Previously, there were tickets labeled as "untriaged," meaning they did not have assigned categories. We improved the system by classifying these tickets without categories. The identified components/types are used as features for the classification process.

### Few-Shot Training with SetFit
We utilized SetFit, a prompt-free few-shot training method for transformers, in EmberV1, a model featured on the MTEB leaderboard. SetFit employs the following pipeline:

#### Contrastive Fine-Tuning
- **Sentence Transformer (ST)**: Fine-tuned on a small number of labeled examples (typically 8 or 16 per class) in a contrastive manner.
- **Definition**: This approach creates positive and negative pairs of sentences. A pair is positive if both sentences belong to the same class, and negative otherwise.
- **Classification Head**: A logistic regression classifier is trained using embeddings generated by the fine-tuned ST to classify text based on these refined features.

This method effectively classifies tickets into their respective categories.

### Lightweight Fine-Tuning with PEFT
To enhance the embedding generation, we implemented lightweight fine-tuning using Parameter-Efficient Fine-Tuning (PEFT). Specifically, we used prefix tuning as our PEFT method:

- **Prefix Tuning**: 
  - Freezes the main parameters of the language model while fine-tuning small, task-specific vector parameters called prefixes.
  - These prefixes are added to the input of each transformer block during training, guiding the model's behavior on specific tasks.
- **Feature Extraction**: We use this task type in PEFT, suitable for embedding models as it provides hidden states (internal representations of the input data at various stages within the model).

### Data Organization and Logistic Regression Model
We organized the data into categories like issue1, issue2, and identified them as duplicate or non-duplicate. We trained a logistic regression model to classify pairs as duplicates or non-duplicates by subtracting embeddings from each other and feeding them into the logistic regression model. We are also comparing this with a cosine similarity scores threshold method.

### Comparison of PEFT and SetFit
- **SetFit**: The prefix parameters are inserted in all model layers, while prompt tuning only adds prompt parameters to the model input embeddings.

### Ensemble Learning Method
We are experimenting with an ensemble learning method instead of disregarding two tickets with different classes:

#### SetFit Classifier for Ticket Types
- **Purpose**: Classifies tickets into categories.
- **Output**: Category labels for each ticket.

#### Fine-Tuned Cosine Similarity Scoring
- **Output**: Cosine similarity scores for each pair.

#### Combining Predictions
We use a meta-learner approach:
- **Step 1**: Obtain predictions from the SetFit classifier for each ticket.
- **Step 2**: Calculate the cosine similarity score for each ticket pair.
- **Step 3**: Combine these predictions in a meta-learner framework.

#### Penalizing Non-Matching Classes
If the SetFit classifier assigns different categories to the tickets in a pair, the meta-learner penalizes the similarity score, indicating a lower likelihood of being duplicates.

#### Boosting Matching Classes
If the SetFit classifier assigns the same category to both tickets, the meta-learner boosts the similarity score, indicating a higher likelihood of being duplicates.

### Final Model
We train a logistic regression or another suitable model that takes both the cosine similarity score and the category match/mismatch indicator as input features to classify pairs as duplicates or non-duplicates.

