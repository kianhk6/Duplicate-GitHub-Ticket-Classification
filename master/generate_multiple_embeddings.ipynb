{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers.util import cos_sim\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(model_name):\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    \n",
    "    dataset_name = 'mozilla_firefox'\n",
    "    df = pd.read_pickle('../data/' + dataset_name + '.pkl')\n",
    "    \n",
    "    def find_row_by_issue_id(df, issue_id):\n",
    "        return df.loc[df['Issue_id'] == issue_id]\n",
    "\n",
    "    def find_rows_by_issue_ids(dataframe, issue_ids):\n",
    "        \"\"\"\n",
    "        Returns a DataFrame containing only the rows with matching Issue_id.\n",
    "        \n",
    "        :param dataframe: pandas DataFrame containing an 'Issue_id' column\n",
    "        :param issue_ids: List of issue IDs to find\n",
    "        :return: DataFrame with only the rows that have a matching Issue_id\n",
    "        \"\"\"\n",
    "        return dataframe[dataframe['Issue_id'].isin(issue_ids)]\n",
    "    \n",
    "    df_subset_without_duplicates = df.head(10000)\n",
    "    \n",
    "    def insert_randomly(main_df, insert_dfs):\n",
    "        \"\"\"\n",
    "        Inserts the rows from the list of DataFrames (insert_dfs) into the main DataFrame (main_df) at random positions.\n",
    "        \n",
    "        :param main_df: The main DataFrame where the other DataFrames are inserted.\n",
    "        :param insert_dfs: A list of DataFrames to insert into main_df.\n",
    "        :return: A new DataFrame with the inserted rows at random positions.\n",
    "        \"\"\"\n",
    "        # Concatenate all the duplicates DataFrames into one for easier manipulation\n",
    "        df_to_insert = pd.concat(insert_dfs).reset_index(drop=True)\n",
    "        \n",
    "        # Calculate the insertion points\n",
    "        insertion_points = np.random.randint(0, len(main_df), len(df_to_insert))\n",
    "        \n",
    "        # Iterate through the insertion points and insert the rows\n",
    "        for insertion_point, row_to_insert in zip(insertion_points, df_to_insert.iterrows()):\n",
    "            part1 = main_df.iloc[:insertion_point]\n",
    "            part2 = main_df.iloc[insertion_point:]\n",
    "            main_df = pd.concat([part1, pd.DataFrame([row_to_insert[1]]), part2], ignore_index=True)\n",
    "        \n",
    "        return main_df\n",
    "    \n",
    "    duplicates1 = find_rows_by_issue_ids(df, [335186, 334862])\n",
    "    duplicates2 = find_rows_by_issue_ids(df, [254967, 265118, 265103])\n",
    "    duplicates3 = find_rows_by_issue_ids(df, [324801, 205129, 215031])\n",
    "    duplicates4 = find_rows_by_issue_ids(df,[227241, 172962])\n",
    "    duplicates5 = find_rows_by_issue_ids(df,[587440, 407981, 675541, 757056, 647655, 295372, 280509, 413211, 409895, 301776, 310261, 328159, 449385, 449221, 587440, 269207, 274631])\n",
    "\n",
    "    # Concatenate all DataFrames\n",
    "    # df_combined = pd.concat([df_subset_without_duplicates, duplicates1, duplicates2, duplicates3, duplicates4, duplicates5], axis=0)\n",
    "\n",
    "\n",
    "    # randomize order of duplicates to have it more natural \n",
    "    # Use the function to insert duplicates into df_subset_without_duplicates\n",
    "    df_subset = insert_randomly(df_subset_without_duplicates, [duplicates1, duplicates2, duplicates3, duplicates4, duplicates5])\n",
    "    \n",
    "    df_subset.to_pickle('../data/' + dataset_name + '_subset.pkl')\n",
    "    \n",
    "    def generate_embeddings(content, model, issue_id):\n",
    "        \"\"\"Generate embeddings for a given piece of text.\"\"\"\n",
    "        \n",
    "        embedding = model.encode(content, convert_to_tensor=True)\n",
    "\n",
    "        return embedding.cpu().numpy()\n",
    "    \n",
    "    embeddings_df = pd.DataFrame()\n",
    "\n",
    "    embeddings_df['Embedding'] = df_subset.apply(lambda row: generate_embeddings(f\"{row['Title']} {row['Content']}\" if pd.notna(row['Content']) else row['Title'], model=model, issue_id=row['Issue_id']), axis=1)\n",
    "\n",
    "    embeddings_df['Issue_id'] = df_subset['Issue_id']\n",
    "    embeddings_df['Duplicated_issues'] = df_subset['Duplicated_issues']\n",
    "\n",
    "    def typecast_df(df):\n",
    "        df['Duplicated_issues'] = df['Duplicated_issues'].apply(lambda x: [int(i) for i in x])\n",
    "        df[\"Issue_id\"] = df[\"Issue_id\"].astype('Int64')\n",
    "        return df\n",
    "    embeddings_df = typecast_df(embeddings_df)\n",
    "\n",
    "    filename = '../data/' + dataset_name + '_embeddings_' + model_name + '.pkl'\n",
    "    directory = os.path.dirname(filename)\n",
    "\n",
    "    os.makedirs(directory, exist_ok=True) # Create the directory if it doesn't exist\n",
    "    embeddings_df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "generate_embeddings(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-distilroberta-v1\"\n",
    "generate_embeddings(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "generate_embeddings(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "generate_embeddings(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "generate_embeddings(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"avsolatorio/GIST-large-Embedding-v0\"\n",
    "generate_embeddings(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"llmrails/ember-v1\"\n",
    "generate_embeddings(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
